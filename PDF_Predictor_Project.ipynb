{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "82OWTF_9c5d7",
        "outputId": "f840045d-9f5d-4bfe-8a61-409137ac47c5"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "üì§ Upload a ZIP containing multiple PDFs:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7039adc1-8bcc-4cea-935c-10e49cf6e16f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7039adc1-8bcc-4cea-935c-10e49cf6e16f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Google Colab Optimized PDF Processing with Threading\n",
        "# This replaces your original sequential processing with parallel threading\n",
        "\n",
        "!pip install fpdf pymupdf ipywidgets psutil --quiet\n",
        "\n",
        "from google.colab import files\n",
        "import zipfile, fitz, pandas as pd\n",
        "from fpdf import FPDF\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import ipywidgets as widgets\n",
        "import base64\n",
        "import time\n",
        "import threading\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import queue\n",
        "import io\n",
        "import gc\n",
        "import psutil\n",
        "import os\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import json\n",
        "\n",
        "class CoLabPDFProcessor:\n",
        "    \"\"\"Optimized PDF processor for Google Colab\"\"\"\n",
        "\n",
        "    def __init__(self, zip_path: str):\n",
        "        self.zip_path = zip_path\n",
        "        self.results_queue = queue.Queue()\n",
        "        self.progress_queue = queue.Queue()\n",
        "        self.failed_files = []\n",
        "        self.processed_count = 0\n",
        "        self.start_time = None\n",
        "\n",
        "    def get_system_info(self):\n",
        "        \"\"\"Get Colab system information\"\"\"\n",
        "        cpu_count = os.cpu_count()\n",
        "        memory_gb = psutil.virtual_memory().total / (1024**3)\n",
        "        return {\n",
        "            'cpu_cores': cpu_count,\n",
        "            'memory_gb': round(memory_gb, 1),\n",
        "            'recommended_workers': min(cpu_count * 2, 12)  # Cap at 12 for Colab\n",
        "        }\n",
        "\n",
        "    def extract_features(self, text: str) -> Dict:\n",
        "        \"\"\"Extract text features with error handling\"\"\"\n",
        "        if not text or not text.strip():\n",
        "            return {\n",
        "                \"word_count\": 0,\n",
        "                \"char_count\": 0,\n",
        "                \"line_count\": 1,\n",
        "                \"avg_word_length\": 0\n",
        "            }\n",
        "\n",
        "        words = text.split()\n",
        "        return {\n",
        "            \"word_count\": len(words),\n",
        "            \"char_count\": len(text),\n",
        "            \"line_count\": max(1, text.count('\\n') + 1),\n",
        "            \"avg_word_length\": sum(len(w) for w in words) / max(1, len(words))\n",
        "        }\n",
        "\n",
        "    def extract_text_safe(self, pdf_data: bytes, filename: str = \"\") -> str:\n",
        "        \"\"\"Safely extract text from PDF bytes\"\"\"\n",
        "        try:\n",
        "            doc = fitz.open(\"pdf\", pdf_data)\n",
        "            text = \"\"\n",
        "            for page in doc:\n",
        "                text += page.get_text()\n",
        "            doc.close()\n",
        "            return text\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è PDF parsing error for {filename}: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def does_fit_one_page(self, content: str, filename: str = \"\") -> int:\n",
        "        \"\"\"Check if content fits one page using FPDF simulation\"\"\"\n",
        "        if not content.strip():\n",
        "            return 1\n",
        "\n",
        "        try:\n",
        "            pdf = FPDF()\n",
        "            pdf.add_page()\n",
        "            pdf.set_font(\"Arial\", size=12)\n",
        "            # Handle encoding issues\n",
        "            safe_content = content.encode('latin-1', 'ignore').decode('latin-1')\n",
        "            pdf.multi_cell(0, 10, safe_content)\n",
        "            result = int(pdf.page_no() == 1)\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            # Fallback: estimate based on character count\n",
        "            return int(len(content) < 2000)\n",
        "\n",
        "    def process_single_pdf(self, pdf_info: Tuple[str, bytes]) -> Optional[Dict]:\n",
        "        \"\"\"Process a single PDF file\"\"\"\n",
        "        pdf_name, pdf_data = pdf_info\n",
        "\n",
        "        try:\n",
        "            # Skip extremely large files to avoid memory issues\n",
        "            if len(pdf_data) > 15 * 1024 * 1024:  # 15MB limit for Colab\n",
        "                self.failed_files.append(f\"{pdf_name} (too large: {len(pdf_data)/(1024*1024):.1f}MB)\")\n",
        "                return None\n",
        "\n",
        "            # Extract text\n",
        "            text = self.extract_text_safe(pdf_data, pdf_name)\n",
        "\n",
        "            # Extract features\n",
        "            features = self.extract_features(text)\n",
        "\n",
        "            # Check if fits one page\n",
        "            fits_one_page = self.does_fit_one_page(text, pdf_name)\n",
        "\n",
        "            # Combine results\n",
        "            result = {\n",
        "                'pdf_name': pdf_name,\n",
        "                'word_count': features['word_count'],\n",
        "                'char_count': features['char_count'],\n",
        "                'line_count': features['line_count'],\n",
        "                'avg_word_length': features['avg_word_length'],\n",
        "                'label': fits_one_page,\n",
        "                'file_size_mb': len(pdf_data) / (1024 * 1024),\n",
        "                'thread_id': threading.current_thread().name\n",
        "            }\n",
        "\n",
        "            # Update progress\n",
        "            self.processed_count += 1\n",
        "            self.progress_queue.put(self.processed_count)\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"{pdf_name}: {str(e)}\"\n",
        "            self.failed_files.append(error_msg)\n",
        "            return None\n",
        "\n",
        "    def progress_monitor(self, total_files: int, update_interval: float = 2.0):\n",
        "        \"\"\"Monitor and display progress in a separate thread\"\"\"\n",
        "        last_update = time.time()\n",
        "\n",
        "        while self.processed_count < total_files:\n",
        "            try:\n",
        "                # Non-blocking check for progress updates\n",
        "                current_count = self.progress_queue.get_nowait()\n",
        "\n",
        "                # Update display every few seconds\n",
        "                current_time = time.time()\n",
        "                if current_time - last_update >= update_interval:\n",
        "                    elapsed = current_time - self.start_time\n",
        "                    rate = current_count / elapsed if elapsed > 0 else 0\n",
        "                    remaining = (total_files - current_count) / rate if rate > 0 else 0\n",
        "\n",
        "                    # Memory usage\n",
        "                    memory_percent = psutil.virtual_memory().percent\n",
        "\n",
        "                    print(f\"\\rüîÑ Progress: {current_count}/{total_files} \"\n",
        "                          f\"({current_count/total_files*100:.1f}%) | \"\n",
        "                          f\"Rate: {rate:.1f} files/sec | \"\n",
        "                          f\"ETA: {remaining/60:.1f}min | \"\n",
        "                          f\"Memory: {memory_percent:.1f}%\",\n",
        "                          end=\"\", flush=True)\n",
        "\n",
        "                    last_update = current_time\n",
        "\n",
        "            except queue.Empty:\n",
        "                time.sleep(0.5)\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                break\n",
        "\n",
        "        print()  # New line when done\n",
        "\n",
        "    def process_all_pdfs_threaded(self, max_workers: int = None) -> pd.DataFrame:\n",
        "        \"\"\"Process all PDFs using ThreadPoolExecutor\"\"\"\n",
        "\n",
        "        # Get system info and determine optimal workers\n",
        "        sys_info = self.get_system_info()\n",
        "        if max_workers is None:\n",
        "            max_workers = sys_info['recommended_workers']\n",
        "\n",
        "        print(f\"üñ•Ô∏è System Info: {sys_info['cpu_cores']} cores, {sys_info['memory_gb']}GB RAM\")\n",
        "        print(f\"üîß Using {max_workers} worker threads\")\n",
        "\n",
        "        # Load PDF files from ZIP\n",
        "        print(\"üìÇ Loading PDF files from ZIP...\")\n",
        "        pdf_data_list = []\n",
        "\n",
        "        try:\n",
        "            with zipfile.ZipFile(self.zip_path, 'r') as z:\n",
        "                pdf_files = [f for f in z.namelist()\n",
        "                           if f.endswith(\".pdf\") and not f.startswith(\"__MACOSX\")]\n",
        "\n",
        "                if not pdf_files:\n",
        "                    raise ValueError(\"No PDF files found in ZIP\")\n",
        "\n",
        "                print(f\"üìö Found {len(pdf_files)} PDF files\")\n",
        "\n",
        "                # Load PDF data into memory (with size filtering)\n",
        "                for pdf_name in pdf_files:\n",
        "                    try:\n",
        "                        file_info = z.getinfo(pdf_name)\n",
        "                        if file_info.file_size > 20 * 1024 * 1024:  # Skip >20MB files\n",
        "                            self.failed_files.append(f\"{pdf_name} (too large: {file_info.file_size/(1024*1024):.1f}MB)\")\n",
        "                            continue\n",
        "\n",
        "                        with z.open(pdf_name) as pdf_file:\n",
        "                            pdf_data = pdf_file.read()\n",
        "                            pdf_data_list.append((pdf_name, pdf_data))\n",
        "\n",
        "                    except Exception as e:\n",
        "                        self.failed_files.append(f\"{pdf_name}: {str(e)}\")\n",
        "                        continue\n",
        "\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Failed to read ZIP file: {e}\")\n",
        "\n",
        "        if not pdf_data_list:\n",
        "            raise ValueError(\"No valid PDF files to process\")\n",
        "\n",
        "        print(f\"‚úÖ Loaded {len(pdf_data_list)} PDFs for processing\")\n",
        "\n",
        "        # Start processing with threading\n",
        "        self.start_time = time.time()\n",
        "        results = []\n",
        "\n",
        "        # Start progress monitor in background\n",
        "        progress_thread = threading.Thread(\n",
        "            target=self.progress_monitor,\n",
        "            args=(len(pdf_data_list),),\n",
        "            daemon=True\n",
        "        )\n",
        "        progress_thread.start()\n",
        "\n",
        "        # Process PDFs with ThreadPoolExecutor\n",
        "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "            # Submit all tasks\n",
        "            future_to_pdf = {\n",
        "                executor.submit(self.process_single_pdf, pdf_data): pdf_data[0]\n",
        "                for pdf_data in pdf_data_list\n",
        "            }\n",
        "\n",
        "            # Collect results as they complete\n",
        "            for future in as_completed(future_to_pdf):\n",
        "                pdf_name = future_to_pdf[future]\n",
        "                try:\n",
        "                    result = future.result()\n",
        "                    if result is not None:\n",
        "                        results.append(result)\n",
        "                except Exception as e:\n",
        "                    self.failed_files.append(f\"{pdf_name}: {str(e)}\")\n",
        "\n",
        "        # Final progress update\n",
        "        processing_time = time.time() - self.start_time\n",
        "\n",
        "        print(f\"\\n‚úÖ Threading completed in {processing_time:.2f} seconds\")\n",
        "        print(f\"üìä Successfully processed: {len(results)}\")\n",
        "        print(f\"‚ùå Failed files: {len(self.failed_files)}\")\n",
        "\n",
        "        if self.failed_files:\n",
        "            print(\"‚ö†Ô∏è First 5 failed files:\")\n",
        "            for failed in self.failed_files[:5]:\n",
        "                print(f\"   - {failed}\")\n",
        "            if len(self.failed_files) > 5:\n",
        "                print(f\"   ... and {len(self.failed_files) - 5} more\")\n",
        "\n",
        "        # Memory cleanup\n",
        "        del pdf_data_list\n",
        "        gc.collect()\n",
        "\n",
        "        return pd.DataFrame(results) if results else pd.DataFrame()\n",
        "\n",
        "# ---- MAIN EXECUTION PIPELINE ----\n",
        "\n",
        "# Step 1: Upload ZIP with progress indication\n",
        "print(\"üì§ Upload a ZIP containing multiple PDFs:\")\n",
        "try:\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        raise ValueError(\"No files uploaded\")\n",
        "    zip_path = next(iter(uploaded))\n",
        "    print(f\"‚úÖ Uploaded: {zip_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Upload failed: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Step 2: Initialize processor and get system info\n",
        "processor = CoLabPDFProcessor(zip_path)\n",
        "sys_info = processor.get_system_info()\n",
        "\n",
        "print(f\"\\nüîß Colab Environment:\")\n",
        "print(f\"   ‚Ä¢ CPU cores: {sys_info['cpu_cores']}\")\n",
        "print(f\"   ‚Ä¢ Memory: {sys_info['memory_gb']} GB\")\n",
        "print(f\"   ‚Ä¢ Recommended workers: {sys_info['recommended_workers']}\")\n",
        "\n",
        "# Step 3: Process PDFs with threading\n",
        "print(f\"\\nüöÄ Starting threaded processing...\")\n",
        "print(f\"‚è±Ô∏è Estimated time: 3-8 minutes for 1076 files\")\n",
        "\n",
        "start_total = time.time()\n",
        "\n",
        "try:\n",
        "    df = processor.process_all_pdfs_threaded()\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"‚ùå No PDFs could be processed\")\n",
        "        exit()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Processing failed: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Step 4: Save dataset and train model\n",
        "print(f\"\\nüíæ Saving dataset...\")\n",
        "df.to_csv(\"threaded_pdf_dataset.csv\", index=False)\n",
        "files.download(\"threaded_pdf_dataset.csv\")\n",
        "\n",
        "print(f\"üìä Dataset Summary:\")\n",
        "print(f\"   ‚Ä¢ Total files processed: {len(df)}\")\n",
        "print(f\"   ‚Ä¢ Files that fit (1): {sum(df['label'] == 1)}\")\n",
        "print(f\"   ‚Ä¢ Files that exceed (0): {sum(df['label'] == 0)}\")\n",
        "print(f\"   ‚Ä¢ Average file size: {df['file_size_mb'].mean():.2f} MB\")\n",
        "\n",
        "# Step 5: Train model\n",
        "print(f\"\\nü§ñ Training RandomForest model...\")\n",
        "X = df.drop(columns=['label', 'pdf_name', 'file_size_mb', 'thread_id'])\n",
        "y = df['label']\n",
        "\n",
        "if len(df) < 2:\n",
        "    X_train, X_test, y_train, y_test = X, X, y, y\n",
        "    test_accuracy = None\n",
        "else:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Use more trees since we have threading power\n",
        "model = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "train_accuracy = model.score(X_train, y_train)\n",
        "test_accuracy = model.score(X_test, y_test) if len(X_test) > 0 else None\n",
        "\n",
        "print(f\"‚úÖ Model trained:\")\n",
        "print(f\"   ‚Ä¢ Training accuracy: {train_accuracy:.3f}\")\n",
        "if test_accuracy:\n",
        "    print(f\"   ‚Ä¢ Test accuracy: {test_accuracy:.3f}\")\n",
        "\n",
        "# Step 6: Generate complete results\n",
        "print(f\"\\nüìã Generating complete results...\")\n",
        "X_all = df.drop(columns=['label', 'pdf_name', 'file_size_mb', 'thread_id'])\n",
        "predictions = model.predict(X_all)\n",
        "\n",
        "complete_results = []\n",
        "for i, row in df.iterrows():\n",
        "    complete_results.append({\n",
        "        'pdf_name': row['pdf_name'],\n",
        "        'word_count': int(row['word_count']),\n",
        "        'char_count': int(row['char_count']),\n",
        "        'line_count': int(row['line_count']),\n",
        "        'avg_word_length': round(row['avg_word_length'], 2),\n",
        "        'file_size_mb': round(row['file_size_mb'], 3),\n",
        "        'actual_label': int(row['label']),\n",
        "        'predicted_label': int(predictions[i]),\n",
        "        'actual_result': 'Fits' if row['label'] == 1 else 'Exceeds',\n",
        "        'predicted_result': 'Fits' if predictions[i] == 1 else 'Exceeds',\n",
        "        'prediction_correct': 'Yes' if row['label'] == predictions[i] else 'No',\n",
        "        'processed_by_thread': row['thread_id']\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(complete_results)\n",
        "results_df.to_csv(\"threaded_pdf_results.csv\", index=False)\n",
        "files.download(\"threaded_pdf_results.csv\")\n",
        "\n",
        "# Final summary\n",
        "total_time = time.time() - start_total\n",
        "accuracy = sum(results_df['prediction_correct'] == 'Yes') / len(results_df)\n",
        "\n",
        "print(f\"\\nüéâ THREADED PROCESSING COMPLETE!\")\n",
        "print(f\"‚è±Ô∏è Total time: {total_time/60:.1f} minutes (vs 15-25 min sequential)\")\n",
        "print(f\"‚ö° Speed improvement: {15*60/total_time:.1f}x faster\")\n",
        "print(f\"üìÅ Results: threaded_pdf_results.csv\")\n",
        "print(f\"üìä Final accuracy: {accuracy:.2%}\")\n",
        "print(f\"üîß Files processed by {sys_info['recommended_workers']} threads\")\n",
        "\n",
        "# Enhanced Interactive UI with threading info\n",
        "pdf_input = widgets.Text(\n",
        "    placeholder='Type exact PDF name',\n",
        "    description='üìÑ PDF Name:',\n",
        "    layout=widgets.Layout(width='70%')\n",
        ")\n",
        "\n",
        "predict_button = widgets.Button(description=\"üîç Predict\", button_style='primary')\n",
        "show_threads_button = widgets.Button(description=\"üßµ Show Thread Stats\", button_style='info')\n",
        "show_all_button = widgets.Button(description=\"üìö Show All Results\", button_style='success')\n",
        "\n",
        "display(pdf_input, predict_button, show_threads_button, show_all_button)\n",
        "\n",
        "output_area = widgets.Output()\n",
        "display(output_area)\n",
        "\n",
        "# Threading statistics\n",
        "def show_thread_stats(b):\n",
        "    with output_area:\n",
        "        output_area.clear_output()\n",
        "\n",
        "        thread_stats = df.groupby('thread_id').agg({\n",
        "            'pdf_name': 'count',\n",
        "            'file_size_mb': ['mean', 'sum'],\n",
        "            'label': 'mean'\n",
        "        }).round(3)\n",
        "\n",
        "        print(\"üßµ THREADING PERFORMANCE STATISTICS\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Total threads used: {df['thread_id'].nunique()}\")\n",
        "        print(f\"Average files per thread: {len(df) / df['thread_id'].nunique():.1f}\")\n",
        "        print(f\"Processing time: {total_time:.1f} seconds\")\n",
        "        print(f\"Throughput: {len(df) / total_time:.1f} files/second\")\n",
        "        print(\"\\nPer-thread breakdown:\")\n",
        "        print(thread_stats)\n",
        "\n",
        "def predict_single(b):\n",
        "    with output_area:\n",
        "        output_area.clear_output(wait=True)\n",
        "\n",
        "        pdf_name = pdf_input.value.strip()\n",
        "        if not pdf_name:\n",
        "            print(\"‚ùå Please enter a PDF name\")\n",
        "            return\n",
        "\n",
        "        # Find in results\n",
        "        match = results_df[results_df['pdf_name'].str.contains(pdf_name, case=False, na=False)]\n",
        "\n",
        "        if match.empty:\n",
        "            print(f\"‚ùå '{pdf_name}' not found in processed results\")\n",
        "            return\n",
        "\n",
        "        row = match.iloc[0]\n",
        "        print(f\"üìò PDF: {row['pdf_name']}\")\n",
        "        print(f\"üìè Features: {row['word_count']} words, {row['line_count']} lines\")\n",
        "        print(f\"üì¶ File size: {row['file_size_mb']} MB\")\n",
        "        print(f\"ü§ñ Model prediction: {'‚úÖ Fits' if row['predicted_label'] == 1 else '‚ùå Exceeds'}\")\n",
        "        print(f\"üßæ Actual result: {'‚úÖ Fits' if row['actual_label'] == 1 else '‚ùå Exceeds'}\")\n",
        "        print(f\"üßµ Processed by: {row['processed_by_thread']}\")\n",
        "        print(f\"‚úÖ Prediction {'correct' if row['prediction_correct'] == 'Yes' else 'incorrect'}\")\n",
        "\n",
        "def show_all_results(b):\n",
        "    with output_area:\n",
        "        output_area.clear_output()\n",
        "\n",
        "        print(f\"üìä COMPLETE RESULTS SUMMARY\")\n",
        "        print(f\"Total processed: {len(results_df)}\")\n",
        "        print(f\"Accuracy: {accuracy:.2%}\")\n",
        "        print(f\"Processing speed: {len(results_df)/total_time:.1f} files/sec\")\n",
        "        print(\"\\nFirst 10 results:\")\n",
        "\n",
        "        display_cols = ['pdf_name', 'word_count', 'predicted_result', 'actual_result', 'prediction_correct']\n",
        "        display(results_df[display_cols].head(10))\n",
        "\n",
        "predict_button.on_click(predict_single)\n",
        "show_threads_button.on_click(show_thread_stats)\n",
        "show_all_button.on_click(show_all_results)\n",
        "\n",
        "print(f\"\\nüéØ Ready! Your {len(results_df)} PDFs processed in {total_time/60:.1f} minutes using threading!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}